{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Instructions\n",
    "\n",
    "1. Enter your Name and UID in the provided space.\n",
    "2. Do the assignment in the notebook itself\n",
    "3. you are free to use Google Colab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name:  Paaras Bhandari  \n",
    "UID:  116191739"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used Keras to implement a 2 layer binary classification neural network. Report is at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for data processing and analysis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "import math\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import History "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - Cat vs Non-Cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_file, test_file):   # Code from HW2\n",
    "    # Load the training data\n",
    "    train_dataset = h5py.File(train_file, \"r\")\n",
    "    \n",
    "    # Separate features(x) and labels(y) for training set\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])\n",
    "\n",
    "    # Load the test data\n",
    "    test_dataset = h5py.File(test_file, \"r\")\n",
    "    \n",
    "    # Separate features(x) and labels(y) for training set\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file=\"data/train_catvnoncat.h5\"    #  loading training dataset\n",
    "test_file=\"data/test_catvnoncat.h5\"      #  loading test datatset\n",
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data(train_file, test_file)  # seperating input and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-processing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "def buildMode(layers_dim):\n",
    "    (n_x, n_h, n_y) = layers_dim\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_h, input_dim=n_x, activation='relu'))    # adding first hidden layer with relu activation\n",
    "    model.add(Dense(n_y, activation='sigmoid'))                # adding output layer with sigmoid output\n",
    "    \n",
    "    return model\n",
    "\n",
    "model1 = buildMode((12288, 10, 1)) # n_h = 10\n",
    "\n",
    "## using stochastic gradient descent with an initial learning rate of 0.2\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD(lr=0.02)\n",
    "model1.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 epochs: 0.711380124092102 | Accuracy: 0.43540668\n",
      "Loss after 100 epochs: 0.5643119812011719 | Accuracy: 0.75598085\n",
      "Loss after 200 epochs: 0.4663430452346802 | Accuracy: 0.8229665\n",
      "Loss after 300 epochs: 0.36939671635627747 | Accuracy: 0.861244\n",
      "Loss after 400 epochs: 0.3206654489040375 | Accuracy: 0.8755981\n",
      "Loss after 500 epochs: 0.24638144671916962 | Accuracy: 0.9186603\n",
      "Loss after 600 epochs: 0.13446632027626038 | Accuracy: 0.98564595\n",
      "Loss after 700 epochs: 0.08569730073213577 | Accuracy: 0.9952153\n",
      "Loss after 800 epochs: 0.05594748631119728 | Accuracy: 0.9952153\n",
      "Loss after 900 epochs: 0.03838104382157326 | Accuracy: 1.0\n",
      "Loss after 1000 epochs: 0.02819713018834591 | Accuracy: 1.0\n",
      "Loss after 1100 epochs: 0.02181633561849594 | Accuracy: 1.0\n",
      "Loss after 1200 epochs: 0.017559180036187172 | Accuracy: 1.0\n",
      "Loss after 1300 epochs: 0.014531448483467102 | Accuracy: 1.0\n",
      "Loss after 1400 epochs: 0.012306074611842632 | Accuracy: 1.0\n",
      "Loss after 1500 epochs: 0.010609985329210758 | Accuracy: 1.0\n",
      "Loss after 1600 epochs: 0.009285074658691883 | Accuracy: 1.0\n",
      "Loss after 1700 epochs: 0.008216540329158306 | Accuracy: 1.0\n",
      "Loss after 1800 epochs: 0.00735136866569519 | Accuracy: 1.0\n",
      "Loss after 1900 epochs: 0.006640611216425896 | Accuracy: 1.0\n",
      "Loss after 2000 epochs: 0.006034592166543007 | Accuracy: 1.0\n",
      "Loss after 2100 epochs: 0.005526149179786444 | Accuracy: 1.0\n",
      "Loss after 2200 epochs: 0.00508833397179842 | Accuracy: 1.0\n",
      "Loss after 2300 epochs: 0.004707040265202522 | Accuracy: 1.0\n",
      "Loss after 2400 epochs: 0.004374540410935879 | Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# function for printing loss and accuracy after every 100 epochs\n",
    "def print_test_loss(history):\n",
    "    \n",
    "    loss = history.history[\"loss\"]\n",
    "    accuracy = history.history[\"accuracy\"]\n",
    "    \n",
    "    for i in range (0,len(loss),100):\n",
    "        print(\"Loss after \" +str(i) + \" epochs: \" +str(loss[i]) + \" | Accuracy: \"+str(accuracy[i]))\n",
    "\n",
    "history = History()\n",
    "history = model1.fit(train_x.T, train_y.T, verbose=0, epochs=2500, batch_size=209, shuffle=False)\n",
    "print_test_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 625us/step\n",
      "Accuracy: 80.00\n"
     ]
    }
   ],
   "source": [
    "# evaluate the keras model\n",
    "_, accuracy = model1.evaluate(test_x.T, test_y.T)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - IMDB Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_file, test_file):\n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "    \n",
    "    # Read the training dataset file line by line\n",
    "    for line in open(train_file, 'r'):\n",
    "        train_dataset.append(line.strip())\n",
    "        \n",
    "    for line in open(test_file, 'r'):\n",
    "        test_dataset.append(line.strip())\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"data/train_imdb.txt\"\n",
    "test_file = \"data/test_imdb.txt\"\n",
    "train_dataset, test_dataset = load_data(train_file, test_file)\n",
    "y = [1 if i < len(train_dataset)*0.5 else 0 for i in range(len(train_dataset))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-Processing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    \n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "train_dataset_clean = preprocess_reviews(train_dataset)\n",
    "test_dataset_clean = preprocess_reviews(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(binary=True, stop_words=\"english\", max_features=2000)\n",
    "cv.fit(train_dataset_clean)\n",
    "X = cv.transform(train_dataset_clean)\n",
    "\n",
    "X_test = cv.transform(test_dataset_clean)\n",
    "X = np.array(X.todense()).astype(float)\n",
    "X_test = np.array(X_test.todense()).astype(float)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, train_size = 0.80\n",
    ")\n",
    "\n",
    "X_train = X_train.T\n",
    "X_val = X_val.T\n",
    "y_train = y_train.reshape(1,-1)\n",
    "y_val = y_val.reshape(1,-1)\n",
    "\n",
    "n_x = X_train.shape[0]    \n",
    "n_h = 10\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)\n",
    "\n",
    "model2 = buildMode(layers_dims)\n",
    "\n",
    "## using stochastic gradient descent with an initial learning rate of 0.2\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD(lr=0.01)\n",
    "model2.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 epochs: 0.699784517288208 | Accuracy: 0.46375\n",
      "Loss after 100 epochs: 0.641793429851532 | Accuracy: 0.6725\n",
      "Loss after 200 epochs: 0.5812547206878662 | Accuracy: 0.7825\n",
      "Loss after 300 epochs: 0.5163273811340332 | Accuracy: 0.84375\n",
      "Loss after 400 epochs: 0.456033855676651 | Accuracy: 0.875\n",
      "Loss after 500 epochs: 0.4040308892726898 | Accuracy: 0.8875\n",
      "Loss after 600 epochs: 0.3599049746990204 | Accuracy: 0.91125\n",
      "Loss after 700 epochs: 0.3221399188041687 | Accuracy: 0.92625\n",
      "Loss after 800 epochs: 0.2897164821624756 | Accuracy: 0.93875\n",
      "Loss after 900 epochs: 0.26175281405448914 | Accuracy: 0.95125\n",
      "Loss after 1000 epochs: 0.23742519319057465 | Accuracy: 0.9525\n",
      "Loss after 1100 epochs: 0.21612773835659027 | Accuracy: 0.9575\n",
      "Loss after 1200 epochs: 0.19734108448028564 | Accuracy: 0.97\n",
      "Loss after 1300 epochs: 0.1806963086128235 | Accuracy: 0.975\n",
      "Loss after 1400 epochs: 0.16590608656406403 | Accuracy: 0.98\n",
      "Loss after 1500 epochs: 0.1527162343263626 | Accuracy: 0.98375\n",
      "Loss after 1600 epochs: 0.14092588424682617 | Accuracy: 0.98625\n",
      "Loss after 1700 epochs: 0.13036030530929565 | Accuracy: 0.98625\n",
      "Loss after 1800 epochs: 0.1208539679646492 | Accuracy: 0.98875\n",
      "Loss after 1900 epochs: 0.11228891462087631 | Accuracy: 0.9925\n",
      "Loss after 2000 epochs: 0.10455211997032166 | Accuracy: 0.99375\n",
      "Loss after 2100 epochs: 0.09754742681980133 | Accuracy: 0.995\n",
      "Loss after 2200 epochs: 0.09119144082069397 | Accuracy: 0.995\n",
      "Loss after 2300 epochs: 0.08541058748960495 | Accuracy: 0.995\n"
     ]
    }
   ],
   "source": [
    "history = History()\n",
    "history = model2.fit(X_train.T, y_train.T, verbose=0, epochs=2400, batch_size=X_train.shape[0], shuffle=True)\n",
    "print_test_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201/201 [==============================] - 0s 561us/step\n",
      "Accuracy: 90.05\n"
     ]
    }
   ],
   "source": [
    "# evaluate the keras model\n",
    "_, accuracy = model2.evaluate(X_val.T, y_val.T)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used Keras library to implement a 2 layer neural network. The first hidden layer uses a ReLU activation and the second layer (final output layer) uses sigmoid. \n",
    "\n",
    "I first tried to train the model with adam optimizer as it was the most reccomended optimizer (from online resources). However, the loss seemed to converge too early before reaching a minima. I also tried to use the\n",
    "adam optimizer with a custom starting learning rate but it didn't show much improvement. I then tried the SGD (Stochastic gradient descent) optimizer, starting with an initial learning rate of 0.1. The training loss was now converging faster than before, so I raised the initial learning rate to 0.2, and then 0.4. At 0.4 initial learning rate, the loss started to fluctuate/oscillate. So I finally decided to use an initial learning rate of 0.2 with the SGD optimizer. \n",
    "\n",
    "I then tried to play aroud with the batch size and started with a batch size of 32. The training was fluctuating, and this also made sense since we learned in the lectures that if decrease the batch size, we trust the gradient less. Hence, as we increase the learning rate, we must also increase the batch size. I then increased the batch size to 64, and the training loss was still fluctuating. Finally, I decided to use the entire dataset in one batch.\n",
    "\n",
    "\n",
    "I also adjusted the size of the hidden layer, by increasing it starting from 5. As I increased the size of the hidden layer, my training seemed to get slower while my test accuracy started to improve. As I increased the size of the hidden layer further, I could see my training accuracy reach 1 and the test accuracy started to decrease. So, I decided to set the size of my hidden layer as 10. \n",
    "\n",
    "Then, I adjusted the epochs by increasing it starting from 600. At first, my model seemed to have a low test accuracy (perhaps underfitting). As I increased the epochs, my final training loss converged and the test accuracy improved. After a certain point, my test accuracy increased to 1.0 and my test accuracy started to decrease (overfitting). I then chose a reasonable number of epochs (2600). \n",
    "\n",
    "After trying out several permutations and combinations of the above hyperparameters, I was able to achieve an accuracy of 80% on the cat vs non-cat example, and 90.5% on the IMDB reviews example. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
